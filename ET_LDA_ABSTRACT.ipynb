{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from googlesearch import search\n",
    "import re\n",
    "import nltk\n",
    "import chardet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSFdata/NSF_CCF.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299916171744574, 'language': ''}\n",
      "NSFdata/NSF_CICI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_CSSI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_DIBBS.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_MRI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299962504897843, 'language': ''}\n",
      "NSFdata/NSF_OAC.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_SI2.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Lets check what encoding we have for our NSFdata files\n",
    "NSF_csv_files = ['NSFdata/NSF_CCF.csv', 'NSFdata/NSF_CICI.csv', 'NSFdata/NSF_CSSI.csv', 'NSFdata/NSF_DIBBS.csv', 'NSFdata/NSF_MRI.csv', 'NSFdata/NSF_OAC.csv', 'NSFdata/NSF_SI2.csv']\n",
    "for file in NSF_csv_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        print(file, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# 1. Read the CSV file and load it into a DataFrame\n",
    "data = pd.read_csv('NSFdata/NSF_DIBBS.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      This project would automate the creation and d...\n",
      "1      The growing number of cyber attacks on the Int...\n",
      "2      Uranium-series geochronology plays a critical ...\n",
      "3      Cybersecurity has become a significant issue t...\n",
      "4      CIF21 DIBBs: Conceptualization of the Social a...\n",
      "                             ...                        \n",
      "125    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "126    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "127    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "128    Current general circulation models (GCMs) have...\n",
      "129    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "projects = data[[\"AwardNumber\", \"Title\", \"NSFOrganization\", \"PrincipalInvestigator\", \"PIEmailAddress\", \"Abstract\"]]\n",
    "print(projects[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_abstract(abstract):\n",
    "    abstract = re.sub('<[^<]+?>', '', abstract)  # Remove HTML tags\n",
    "    abstract = abstract.lower()  # Convert to lowercase\n",
    "    abstract = re.sub(r'\\W+', ' ', abstract)  # Remove special characters and numbers\n",
    "    words = abstract.split()  # Tokenize\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [project, would, automate, creation, data, ana...\n",
      "1      [growing, number, cyber, attack, internet, cri...\n",
      "2      [uranium, series, geochronology, play, critica...\n",
      "3      [cybersecurity, become, significant, issue, pr...\n",
      "4      [cif21, dibbs, conceptualization, social, inno...\n",
      "                             ...                        \n",
      "125    [abstractopp, 9813312, opp, 9813442, opp, 9813...\n",
      "126    [abstractopp, 9813312, opp, 9813442, opp, 9813...\n",
      "127    [abstractopp, 9907197, opp, 9907469, opp, 9907...\n",
      "128    [current, general, circulation, model, gcms, d...\n",
      "129    [abstractopp, 9907197, opp, 9907469, opp, 9907...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3. Process the Abstract column\n",
    "abstracts = projects[\"Abstract\"].apply(preprocess_abstract)\n",
    "print(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<4137 unique tokens: ['000', '1440753', '24', '50', '500']...>\n"
     ]
    }
   ],
   "source": [
    "# 4. Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(abstracts)\n",
    "print(dictionary)\n",
    "# 5. Corpus is a list of bags of words. Each bag-of-words is a list of tuples (term_id, term_frequency).\n",
    "corpus = [dictionary.doc2bow(text) for text in abstracts]\n",
    "\n",
    "# 6. Define the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=8, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new colunm LDA_ABSTRACTS to be classified into keywords based on the LDA model\n",
    "projects[\"LDA_abstract_keywords\"] = projects[\"Abstract\"].apply(lambda x: lda_model[dictionary.doc2bow(preprocess_abstract(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [(3, 0.45789737), (5, 0.5392573)]\n",
      "1        [(4, 0.5909081), (5, 0.4042163)]\n",
      "2                       [(0, 0.99528813)]\n",
      "3      [(1, 0.9782481), (5, 0.017256107)]\n",
      "4       [(1, 0.8115505), (5, 0.18670286)]\n",
      "                      ...                \n",
      "125                      [(6, 0.9942036)]\n",
      "126                      [(6, 0.9942036)]\n",
      "127                      [(7, 0.9928834)]\n",
      "128     [(2, 0.9078556), (7, 0.08861986)]\n",
      "129                     [(7, 0.99288344)]\n",
      "Name: LDA_abstract_keywords, Length: 130, dtype: object\n",
      "0      This project would automate the creation and d...\n",
      "1      The growing number of cyber attacks on the Int...\n",
      "2      Uranium-series geochronology plays a critical ...\n",
      "3      Cybersecurity has become a significant issue t...\n",
      "4      CIF21 DIBBs: Conceptualization of the Social a...\n",
      "                             ...                        \n",
      "125    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "126    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "127    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "128    Current general circulation models (GCMs) have...\n",
      "129    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(projects[\"LDA_abstract_keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BING_API_KEY = \"your_bing_search_api_key_here\"\n",
    "# 7. Defining Search for news articles and other online sources\n",
    "def search_news(title):\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": BING_API_KEY}\n",
    "    params = {\n",
    "        \"q\": f'\"{title}\"',\n",
    "        \"count\": 3,\n",
    "        \"offset\": 0,\n",
    "        \"mkt\": \"en-US\",\n",
    "        \"safesearch\": \"Moderate\",\n",
    "    }\n",
    "    response = requests.get(\"https://api.cognitive.microsoft.com/bing/v7.0/search\", headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()\n",
    "    news_links = [result[\"url\"] for result in search_results[\"webPages\"][\"value\"] if \"news\" in result[\"url\"] or \"article\" in result[\"url\"]]\n",
    "    return ', '.join(news_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3D%2522CIF21%252BDIBBs%253A%252BPD%253A%252BEnhancing%252Band%252BPersonalizing%252BEducational%252BResources%252Bthrough%252BTools%252Bfor%252BExperimentation%2522%26num%3D12%26hl%3Den%26start%3D0&hl=en&q=EgSBc8MrGNPPm6IGIjBbxxQ_lYzXivQv17H2nj_drNflwrGW5vyu_FAJKfnUP6DenYaYvl4cvJeIKQkqYUcyAXI",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m projects[\u001b[39m\"\u001b[39m\u001b[39mNews\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m projects[\u001b[39m\"\u001b[39;49m\u001b[39mTitle\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(search_news)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(projects[\u001b[39m\"\u001b[39m\u001b[39mNews\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ocv4\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ocv4\\lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ocv4\\lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1144\u001b[0m             values,\n\u001b[0;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ocv4\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m, in \u001b[0;36msearch_news\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch_news\u001b[39m(title):\n\u001b[0;32m      3\u001b[0m     query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtitle\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     news_links \u001b[39m=\u001b[39m [j \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m search(query, num_results\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnews\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m j \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m j]\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(news_links)\n",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch_news\u001b[39m(title):\n\u001b[0;32m      3\u001b[0m     query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtitle\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     news_links \u001b[39m=\u001b[39m [j \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m search(query, num_results\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnews\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m j \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m j]\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(news_links)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ocv4\\lib\\site-packages\\googlesearch\\__init__.py:54\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(term, num_results, lang, proxy, advanced, sleep_interval, timeout)\u001b[0m\n\u001b[0;32m     51\u001b[0m start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[39mwhile\u001b[39;00m start \u001b[39m<\u001b[39m num_results:\n\u001b[0;32m     53\u001b[0m     \u001b[39m# Send request\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     resp \u001b[39m=\u001b[39m _req(escaped_term, num_results \u001b[39m-\u001b[39;49m start,\n\u001b[0;32m     55\u001b[0m                 lang, start, proxies, timeout)\n\u001b[0;32m     57\u001b[0m     \u001b[39m# Parse\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(resp\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ocv4\\lib\\site-packages\\googlesearch\\__init__.py:23\u001b[0m, in \u001b[0;36m_req\u001b[1;34m(term, results, lang, start, proxies, timeout)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_req\u001b[39m(term, results, lang, start, proxies, timeout):\n\u001b[0;32m      9\u001b[0m     resp \u001b[39m=\u001b[39m get(\n\u001b[0;32m     10\u001b[0m         url\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.google.com/search\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m         headers\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m     22\u001b[0m     )\n\u001b[1;32m---> 23\u001b[0m     resp\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ocv4\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3D%2522CIF21%252BDIBBs%253A%252BPD%253A%252BEnhancing%252Band%252BPersonalizing%252BEducational%252BResources%252Bthrough%252BTools%252Bfor%252BExperimentation%2522%26num%3D12%26hl%3Den%26start%3D0&hl=en&q=EgSBc8MrGNPPm6IGIjBbxxQ_lYzXivQv17H2nj_drNflwrGW5vyu_FAJKfnUP6DenYaYvl4cvJeIKQkqYUcyAXI"
     ]
    }
   ],
   "source": [
    "projects[\"News\"] = projects[\"Title\"].apply(search_news)\n",
    "print(projects[\"News\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a new DataFrame with the desired columns\n",
    "output = projects.rename(columns={\"Title\": \"Project_title\",\n",
    "                                   \"NSFOrganization\": \"Funding_agency\",\n",
    "                                   \"AwardNumber\": \"Award_number\",\n",
    "                                   \"PrincipalInvestigator\": \"PI_Name\",\n",
    "                                   \"PIEmailAddress\": \"PI_contact\",\n",
    "                                   \"Abstract\": \"Abstract\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

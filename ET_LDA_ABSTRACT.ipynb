{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from googlesearch import search\n",
    "import re\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSFdata/NSF_CCF.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299916171744574, 'language': ''}\n",
      "NSFdata/NSF_CICI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_CSSI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_DIBBS.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_MRI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299962504897843, 'language': ''}\n",
      "NSFdata/NSF_OAC.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_SI2.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Lets check what encoding we have for our NSFdata files\n",
    "NSF_csv_files = ['NSFdata/NSF_CCF.csv', 'NSFdata/NSF_CICI.csv', 'NSFdata/NSF_CSSI.csv', 'NSFdata/NSF_DIBBS.csv', 'NSFdata/NSF_MRI.csv', 'NSFdata/NSF_OAC.csv', 'NSFdata/NSF_SI2.csv']\n",
    "for file in NSF_csv_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        print(file, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the CSV file and load it into a DataFrame\n",
    "data = pd.read_csv('NSFdata/NSF_DIBBS.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      This project would automate the creation and d...\n",
      "1      The growing number of cyber attacks on the Int...\n",
      "2      Uranium-series geochronology plays a critical ...\n",
      "3      Cybersecurity has become a significant issue t...\n",
      "4      CIF21 DIBBs: Conceptualization of the Social a...\n",
      "                             ...                        \n",
      "125    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "126    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "127    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "128    Current general circulation models (GCMs) have...\n",
      "129    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "projects = data[[\"AwardNumber\", \"Title\", \"NSFOrganization\", \"PrincipalInvestigator\", \"PIEmailAddress\", \"Abstract\"]]\n",
    "print(projects[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Process the Abstract column using LDA\n",
    "abstracts = projects[\"Abstract\"].apply(lambda x: re.sub('<[^<]+?>', '', x).split())  # Remove HTML tags and split into words\n",
    "print(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from googlesearch import search\n",
    "import re\n",
    "import nltk\n",
    "import chardet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSFdata/NSF_CCF.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299916171744574, 'language': ''}\n",
      "NSFdata/NSF_CICI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_CSSI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_DIBBS.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_MRI.csv {'encoding': 'ISO-8859-1', 'confidence': 0.7299962504897843, 'language': ''}\n",
      "NSFdata/NSF_OAC.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n",
      "NSFdata/NSF_SI2.csv {'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Lets check what encoding we have for our NSFdata files\n",
    "NSF_csv_files = ['NSFdata/NSF_CCF.csv', 'NSFdata/NSF_CICI.csv', 'NSFdata/NSF_CSSI.csv', 'NSFdata/NSF_DIBBS.csv', 'NSFdata/NSF_MRI.csv', 'NSFdata/NSF_OAC.csv', 'NSFdata/NSF_SI2.csv']\n",
    "for file in NSF_csv_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        print(file, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# 1. Read the CSV file and load it into a DataFrame\n",
    "data = pd.read_csv('NSFdata/NSF_DIBBS.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      This project would automate the creation and d...\n",
      "1      The growing number of cyber attacks on the Int...\n",
      "2      Uranium-series geochronology plays a critical ...\n",
      "3      Cybersecurity has become a significant issue t...\n",
      "4      CIF21 DIBBs: Conceptualization of the Social a...\n",
      "                             ...                        \n",
      "125    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "126    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "127    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "128    Current general circulation models (GCMs) have...\n",
      "129    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "projects = data[[\"AwardNumber\", \"Title\", \"NSFOrganization\", \"PrincipalInvestigator\", \"PIEmailAddress\", \"Abstract\"]]\n",
    "print(projects[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_abstract(abstract):\n",
    "    abstract = re.sub('<[^<]+?>', '', abstract)  # Remove HTML tags\n",
    "    abstract = abstract.lower()  # Convert to lowercase\n",
    "    abstract = re.sub(r'\\W+', ' ', abstract)  # Remove special characters and numbers\n",
    "    words = abstract.split()  # Tokenize\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [project, would, automate, creation, data, ana...\n",
      "1      [growing, number, cyber, attack, internet, cri...\n",
      "2      [uranium, series, geochronology, play, critica...\n",
      "3      [cybersecurity, become, significant, issue, pr...\n",
      "4      [cif21, dibbs, conceptualization, social, inno...\n",
      "                             ...                        \n",
      "125    [abstractopp, 9813312, opp, 9813442, opp, 9813...\n",
      "126    [abstractopp, 9813312, opp, 9813442, opp, 9813...\n",
      "127    [abstractopp, 9907197, opp, 9907469, opp, 9907...\n",
      "128    [current, general, circulation, model, gcms, d...\n",
      "129    [abstractopp, 9907197, opp, 9907469, opp, 9907...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3. Process the Abstract column\n",
    "abstracts = projects[\"Abstract\"].apply(preprocess_abstract)\n",
    "print(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<4137 unique tokens: ['000', '1440753', '24', '50', '500']...>\n"
     ]
    }
   ],
   "source": [
    "# 4. Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(abstracts)\n",
    "print(dictionary)\n",
    "# 5. Corpus is a list of bags of words. Each bag-of-words is a list of tuples (term_id, term_frequency).\n",
    "corpus = [dictionary.doc2bow(text) for text in abstracts]\n",
    "\n",
    "# 6. Define the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=8, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new colunm LDA_ABSTRACTS to be classified into keywords based on the LDA model\n",
    "projects[\"LDA_abstract_keywords\"] = projects[\"Abstract\"].apply(lambda x: lda_model[dictionary.doc2bow(preprocess_abstract(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [(3, 0.45789737), (5, 0.5392573)]\n",
      "1        [(4, 0.5909081), (5, 0.4042163)]\n",
      "2                       [(0, 0.99528813)]\n",
      "3      [(1, 0.9782481), (5, 0.017256107)]\n",
      "4       [(1, 0.8115505), (5, 0.18670286)]\n",
      "                      ...                \n",
      "125                      [(6, 0.9942036)]\n",
      "126                      [(6, 0.9942036)]\n",
      "127                      [(7, 0.9928834)]\n",
      "128     [(2, 0.9078556), (7, 0.08861986)]\n",
      "129                     [(7, 0.99288344)]\n",
      "Name: LDA_abstract_keywords, Length: 130, dtype: object\n",
      "0      This project would automate the creation and d...\n",
      "1      The growing number of cyber attacks on the Int...\n",
      "2      Uranium-series geochronology plays a critical ...\n",
      "3      Cybersecurity has become a significant issue t...\n",
      "4      CIF21 DIBBs: Conceptualization of the Social a...\n",
      "                             ...                        \n",
      "125    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "126    ABSTRACT<br/><br/>OPP-9813312   OPP-9813442   ...\n",
      "127    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "128    Current general circulation models (GCMs) have...\n",
      "129    ABSTRACT<br/><br/>OPP-9907197    OPP-9907469  ...\n",
      "Name: Abstract, Length: 130, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(projects[\"LDA_abstract_keywords\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
